"""
RAG ç³»ç»ŸåŒ…è£…ç±» - ç”¨äº FastAPI
"""

import os
import threading
from pathlib import Path
from dotenv import load_dotenv
from fastapi.responses import StreamingResponse
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

from core.config import DEFAULT_CONFIG, RAGConfig
from rag_modules import (
    DataPreparationModule,
    IndexConstructionModule,
    RetrievalOptimizationModule,
    GenerationIntegrationModule
)

load_dotenv()


class RecipeRAGSystem:
    """é£Ÿè°±RAGç³»ç»ŸåŒ…è£…ç±»"""
    # å•ä¾‹æ¨¡å¼ç›¸å…³å±æ€§
    _instance = None
    _lock = threading.Lock()

    def __new__(cls, config: RAGConfig = None):
        """å®ç°çº¿ç¨‹å®‰å…¨çš„å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            with cls._lock:
                # åŒé‡æ£€æŸ¥ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨
                if cls._instance is None:
                    cls._instance = super(RecipeRAGSystem, cls).__new__(cls)
        return cls._instance

    def __init__(self, config: RAGConfig = None, force_reinit: bool = False):
        # ç¡®ä¿åªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œé™¤éå¼ºåˆ¶é‡æ–°åˆå§‹åŒ–
        if hasattr(self, '_initialized') and not force_reinit:
            return

        self.config = config or DEFAULT_CONFIG
        self.data_module = None
        self.index_module = None
        self.retrieval_module = None
        self.generation_module = None
        # å¼€å§‹åˆå§‹åŒ–æ•°æ®å‡†å¤‡æ¨¡å—
        self.initialize_system()
        print("="*30)
        if not Path(self.config.data_path).exists():
            # åˆå§‹åŒ–æ„å»ºç´¢å¼•å‘é‡
            print(f"æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {self.config.data_path},å¼€å§‹æ„å»ºç´¢å¼•")

        if not os.getenv("LLM_API_KEY"):
            raise ValueError("è¯·è®¾ç½® LLM_API_KEY ç¯å¢ƒå˜é‡")
        print("å¼€å§‹æ„å»ºçŸ¥è¯†åº“")
        self.build_knowledge_base()
        print("çŸ¥è¯†åº“æ„å»ºå®Œæˆ")
        
        # æ ‡è®°ä¸ºå·²åˆå§‹åŒ–
        self._initialized = True


    def initialize_system(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å—"""
        print("åˆå§‹åŒ–æ•°æ®å‡†å¤‡æ¨¡å—...")
        self.data_module = DataPreparationModule(self.config.data_path)
        print("æ•°æ®å‡†å¤‡æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
        print("åˆå§‹åŒ–ç´¢å¼•æ„å»ºæ¨¡å—...")
        self.index_module = IndexConstructionModule(
            model_name=self.config.embedding_model,
            index_save_path=self.config.index_save_path
        )
        print("ç´¢å¼•æ„å»ºæ¨¡å—åˆå§‹åŒ–å®Œæˆ")
        print("ğŸ¤– åˆå§‹åŒ–ç”Ÿæˆé›†æˆæ¨¡å—...")
        self.generation_module = GenerationIntegrationModule(
            model_name=self.config.llm_model,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens
        )
        print("ç”Ÿæˆé›†æˆæ¨¡å—åˆå§‹åŒ–å®Œæˆ")
        print("åˆå§‹åŒ–å®Œæˆ")

    def build_knowledge_base(self):
        """æ„å»ºçŸ¥è¯†åº“"""
        print("å¼€å§‹æ„å»ºçŸ¥è¯†åº“...")
        vectorstore = self.index_module.load_index()

        if vectorstore is not None:
            print("âœ… æˆåŠŸåŠ è½½å·²ä¿å­˜çš„å‘é‡ç´¢å¼•ï¼")
            # ä»éœ€è¦åŠ è½½æ–‡æ¡£å’Œåˆ†å—ç”¨äºæ£€ç´¢æ¨¡å—
            print("åŠ è½½é£Ÿè°±æ–‡æ¡£...")
            self.data_module.load_documents()
            print("è¿›è¡Œæ–‡æœ¬åˆ†å—...")
            chunks = self.data_module.chunk_documents()
        else:
            print("æœªæ‰¾åˆ°å·²ä¿å­˜çš„ç´¢å¼•ï¼Œå¼€å§‹æ„å»ºæ–°ç´¢å¼•...")
            # 2. åŠ è½½æ–‡æ¡£
            print("åŠ è½½é£Ÿè°±æ–‡æ¡£...")
            self.data_module.load_documents()
            # 3. æ–‡æœ¬åˆ†å—
            print("è¿›è¡Œæ–‡æœ¬åˆ†å—...")
            chunks = self.data_module.chunk_documents()
            # 4. æ„å»ºå‘é‡ç´¢å¼•
            print("æ„å»ºå‘é‡ç´¢å¼•...")
            vectorstore = self.index_module.build_vector_index(chunks)
            # 5. ä¿å­˜ç´¢å¼•
            print("ä¿å­˜å‘é‡ç´¢å¼•...")
            self.index_module.save_index()

        print("åˆå§‹åŒ–æ£€ç´¢ä¼˜åŒ–...")
        self.retrieval_module = RetrievalOptimizationModule(
            self.index_module.vectorstore,
            self.data_module.chunks,
            score_threshold=self.config.score_threshold,
            rrf_weights=self.config.rrf_weights
        )

        stats = self.data_module.get_statistics()
        print(f"\nğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡:")
        print(f"   æ–‡æ¡£æ€»æ•°: {stats['total_documents']}")
        print(f"   æ–‡æœ¬å—æ•°: {stats['total_chunks']}")
        print(f"   èœå“åˆ†ç±»: {list(stats['categories'].keys())}")
        print(f"   éš¾åº¦åˆ†å¸ƒ: {stats['difficulties']}")

        print("âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")

    def get_statistics(self):
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        return self.data_module.get_statistics()

    def ask_question(self, question: str, chat_history: list = None, stream: bool = True):
        """å›ç­”ç”¨æˆ·é—®é¢˜"""
        
        # æ ¼å¼åŒ–èŠå¤©å†å²
        chat_history_str = ""
        if chat_history:
            history_parts = []
            for msg in chat_history[-6:]: # åªå–æœ€è¿‘6æ¡
                role_name = "ç”¨æˆ·" if msg.get("role") == "user" else "åŠ©æ‰‹"
                content = msg.get("content", "")
                history_parts.append(f"{role_name}: {content}")
            chat_history_str = "\n".join(history_parts)

        if not all([self.retrieval_module, self.generation_module]):
            raise ValueError("è¯·å…ˆæ„å»ºçŸ¥è¯†åº“")
        print(f"å¼€å§‹å¤„ç†é—®é¢˜:{question},å½“å‰çš„streamä¸º{stream}")
        route_type = self.generation_module.query_router(question)


        print(f"è·¯ç”±ç»“æœ: {route_type}")

        # å¦‚æœæ˜¯é—²èŠï¼Œç›´æ¥ç”Ÿæˆä¸éœ€è¦æ£€ç´¢
        if route_type == 'chat':
            if stream:
                async def generate():
                    answer_chunks = self.generation_module.generate_chat_answer_stream(question, chat_history=chat_history_str)
                    async for chunk in answer_chunks:
                        row = {
                            "answer": chunk,
                            "documents": [],
                            "route_type": "chat"
                        }
                        yield row
                
                return generate()
            else:
                answer = self.generation_module.generate_chat_answer(question, chat_history=chat_history_str)
                return {
                    "answer": answer,
                    "documents": [],
                    "route_type": "chat"
                }

        # ä¼˜åŒ–æŸ¥è¯¢ï¼ˆä»…éé—²èŠï¼‰
        rewritten_query = self.generation_module.query_rewrite(question)

        filters = self._extract_filters_from_query(question)
        if filters:
            relevant_chunks = self.retrieval_module.metadata_filtered_search(
                rewritten_query, filters, top_k=self.config.top_k
            )
        else:
            relevant_chunks = self.retrieval_module.hybrid_search(
                rewritten_query, top_k=self.config.top_k
            )

        if not relevant_chunks:
            if stream:
                async def empty_generator():
                    yield {
                        "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„é£Ÿè°±ä¿¡æ¯ã€‚è¯·å°è¯•å…¶ä»–èœå“åç§°æˆ–å…³é”®è¯ã€‚",
                        "route_type": route_type,
                        "documents": []
                    }
                return empty_generator()
            else:
                return {
                    "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„é£Ÿè°±ä¿¡æ¯ã€‚è¯·å°è¯•å…¶ä»–èœå“åç§°æˆ–å…³é”®è¯ã€‚",
                    "route_type": route_type,
                    "documents": []
                }

        relevant_docs = self.data_module.get_parent_documents(relevant_chunks)

        doc_info = []
        for doc in relevant_docs:
            doc_info.append({
                "dish_name": doc.metadata.get('dish_name', 'æœªçŸ¥èœå“'),
                "category": doc.metadata.get('category', 'æœªçŸ¥'),
                "difficulty": doc.metadata.get('difficulty', 'æœªçŸ¥')
            })

        if stream:
            async def stream_generator():
                if route_type == 'list':
                    answer_chunks = self.generation_module.generate_list_answer_stream(question, relevant_docs)
                elif route_type == "detail":
                    answer_chunks = self.generation_module.generate_step_by_step_answer_stream(question, relevant_docs, chat_history=chat_history_str)
                else:
                    answer_chunks = self.generation_module.generate_basic_answer_stream(question, relevant_docs, chat_history=chat_history_str)
                
                async for chunk in answer_chunks:
                    row = {
                        "answer": chunk,
                        "route_type": route_type,
                        "documents": doc_info
                    }
                    yield row
            
            return stream_generator()
        else:
            if route_type == 'list':
                answer = self.generation_module.generate_list_answer(question, relevant_docs)
            elif route_type == "detail":
                answer = self.generation_module.generate_step_by_step_answer(question, relevant_docs, chat_history=chat_history_str)
            else:
                answer = self.generation_module.generate_basic_answer(question, relevant_docs, chat_history=chat_history_str)

            return {
                "answer": answer,
                "route_type": route_type,
                "documents": doc_info
            }

    def search_by_category(self, category: str, query: str = ""):
        """æŒ‰åˆ†ç±»æœç´¢èœå“"""
        if not self.retrieval_module:
            raise ValueError("è¯·å…ˆæ„å»ºçŸ¥è¯†åº“")

        search_query = query if query else category
        filters = {"category": category}

        docs = self.retrieval_module.metadata_filtered_search(search_query, filters, top_k=10)

        dish_names = []
        for doc in docs:
            dish_name = doc.metadata.get('dish_name', 'æœªçŸ¥èœå“')
            if dish_name not in dish_names:
                dish_names.append(dish_name)

        return dish_names

    def _extract_filters_from_query(self, query: str) -> dict:
        """ä»ç”¨æˆ·é—®é¢˜ä¸­æå–å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶"""
        filters = {}
        category_keywords = DataPreparationModule.get_supported_categories()
        for cat in category_keywords:
            if cat in query:
                filters['category'] = cat
                break

        difficulty_keywords = DataPreparationModule.get_supported_difficulties()
        for diff in sorted(difficulty_keywords, key=len, reverse=True):
            if diff in query:
                filters['difficulty'] = diff
                break

        return filters